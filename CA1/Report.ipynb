{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# $$ Inteligent \\hspace{3mm}Systems - Homework \\hspace{2mm} 1 $$\n",
    "### $$Narjes \\hspace{2mm} Noorzad - 810196626$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Abstract: <br>\n",
    "> In \"Question 1\" we are gonna learn about $stationary \\hspace{2mm} points$, the procedure of finding and classifying them using $Hessian\\hspace{2mm} matrix$ .<br><br>\n",
    "> In \"Question 2\" we are gonna find the  $step size$ using two different approaches and comparing them.<br><br>\n",
    "> In \"Question 3\" we are gonna learn how to classify a multiclass problem using $one\\hspace{2mm} vs. \\hspace{2mm}all$ also avoid overfitting using $L1 \\hspace{2mm}regularization$<br><br>\n",
    "> Finally, in \"Question 4\" we are gonna use KNN to classify"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question1\n",
    ">A stationary point of a function $f(x)$ is a point where the derivative of $f(x)$ is equal to 0. These points are called “stationary” because at these points the function is neither increasing nor decreasing. Graphically, this corresponds to points on the graph of $f(x)$ where the tangent to the curve is a horizontal line. \n",
    ">>**Classifying stationary points**<br><br>\n",
    "The procedure for classifying stationary points of a function of two variables is analogous to, but somewhat more involved, than the corresponding ‘second derivative test’ for functions of one variable.<br> Below is, essentially, the second derivative test for functions of two variables:<br>\n",
    "Let (a, b) be a stationary point, so that $f_x1 = 0$ and $f_x2 = 0$ at (a, b). Then: <br><br>\n",
    "• if $f_{x1x1}f_{x2x2} − f^2_{x1x2} < 0 $ at $(a, b)$ then $(a, b)$ is a saddle point. <br>\n",
    "• if $f_{x1x1}f_{x2x2} − f^2_{x1x2} < 0 $ at $(a, b)$ then $(a, b)$  is either a maximum or a minimum. <br>\n",
    ">>>Distinguish between these as follows:<br><br>\n",
    "• if $f_{x1x1}$ and $f_{x2x2}$ at $(a, b)$ then $(a, b)$ is a maximum point. <br>\n",
    "• if $f_{x1x1}$ and $f_{x2x2}$ at $(a, b)$ then $(a, b)$ is a minimum point. <br>\n",
    "If $f_{x1x1}f_{x2x2} − f^2_{x1x2} = 0 $then anything is possible. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### solution :\n",
    " \n",
    "\n",
    ">$$f(x) = 3x_1^2 + 2x_2^2 - 3x_1x_2 + 4x_1^3 + x_1^4$$\n",
    "<br>\n",
    "$ f_{x_1} =  \\frac{ \\partial f(x)}{\\partial x_1} =6x_1 - 3x_2 + 12x_1^2 + 4x_1^3 $<br><br>\n",
    "$ f_{x_1 x_1} =  \\frac{ \\partial \\partial f(x)}{\\partial x_1 \\partial x_1} =  \\frac{\\partial (6x_1 - 3x_2 + 12x_1^2 + 4x_1^3)}{\\partial x_1}  = 12x_1^2 + 24x_1 + 6 $\n",
    "<br>\n",
    "<br>\n",
    "$ f_{x_2} = \\frac{ \\partial f(x)}{\\partial x_2} = 4x_2 - 3x_1$\n",
    "<br>\n",
    "<br>\n",
    "$ f_{x_2 x_2} = \\frac{ \\partial  \\partial  f(x)}{\\partial x_2 \\partial x_2} = \\frac{\\partial(4x_2 - 3x_1)}{ \\partial x_2} = 4$\n",
    "<br><br>\n",
    "$ f_{x_2 x_1} = \\frac{ \\partial  \\partial f(x)}{\\partial x_2  \\partial x_1 } = \\frac{\\partial(4x_2 - 3x_1)}{ \\partial x_1} = -3$ <br>\n",
    "<br><br>\n",
    "$ f_{x_1} = 0  \\Rightarrow x_2 = 2x_1 + 4x_1^2 + \\frac{4}{3}x_1^3 \\hspace{5mm} \\star $ <br>\n",
    "<br><br>\n",
    "$ f_{x_2} = 0  \\Rightarrow x_2 = \\frac{3}{4}x_1   \\overset{\\star}{\\Rightarrow} \\frac{4}{3}x_1^3 + 4x_1^2 +  \\frac{5}{4}x_1 = 0   \\Rightarrow  x_1(\\frac{4}{3}x_1^2 + 4x_1 +  \\frac{5}{4}) = 0   \\Rightarrow  x_1 = 0, \\hspace{2mm} \\frac{1}{4}(-6- \\sqrt{21}), \\hspace{2mm}   \\frac{1}{4}(\\sqrt{21} - 6)  $\n",
    "<br>\n",
    "<br>\n",
    "$ @ x_1 = 0 \\Rightarrow x_2 = 0  $\n",
    "<br><br>\n",
    "$ @ x_1 = \\frac{1}{4}(-6- \\sqrt{21}) \\Rightarrow x_2 =  \\frac{2}{4}(-6- \\sqrt{21}) +(-6- \\sqrt{21})^2 + \\frac{1}{3}(-6- \\sqrt{21})^3 = \\frac{-3}{16}(6 + \\sqrt{21})   $\n",
    " <br><br>$ @ x_1 =  \\frac{1}{4}(\\sqrt{21} - 6)  \\Rightarrow x_2 =  \\frac{2}{4}(\\sqrt{21} - 6) +( \\sqrt{21} - 6 )^2 + \\frac{1}{3}( \\sqrt{21} - 6 )^3 =   \\frac{3}{16}(\\sqrt{21} - 6)  $ \n",
    "<br>\n",
    "<br>\n",
    " $ f_{x_1 x_1 } f_{x_2 x_2 } - f_{x_1 x_2 }^2  \\text { at } (0, 0) = (6)(4) - (-3)^2 > 0  \\Rightarrow \\textbf{maximum or a minimum} $ \n",
    " <br><br>\n",
    "$ f_{x_1x_1} f_{x_2 x_2} - f_{x_1 x_2 }^2  \\text { at } ( \\frac{1}{4}(-6- \\sqrt{21}) ,  \\frac{-3}{16}(6 + \\sqrt{21}) ) = 12(\\frac{1}{4}(-\\sqrt{21} - 6)^2 + 24(\\frac{1}{4}(-\\sqrt{21} - 6) + 6)(4) - (-3)^2  > 0  \\Rightarrow \\textbf{maximum or a minimum} $ \n",
    "<br><br>\n",
    " $ f_{x_1 x_1 } f_{x_2 x_2 } - f_{x_1 x_2 }^2  \\text { at } ( \\frac{1}{4}(\\sqrt{21} - 6), \\frac{3}{16}(\\sqrt{21} - 6)) = 12(\\frac{1}{4}(\\sqrt{21} - 6)^2 + 24(\\frac{1}{4}(\\sqrt{21} - 6) + 6)(4) - (-3)^2  < 0  \\Rightarrow  \\textbf{saddle point} $ \n",
    "<br><br>\n",
    "Distinguish between maximum or a minimum as follows:\n",
    "<br><br>\n",
    "$ f_{x_1 x_1 }  \\text { at } (0, 0) = 6 > 0  \\hspace{5mm}and  \\hspace{5mm}  f_{x_2 x_2 }  \\text { at } (0, 0) = 4 > 0   \\Rightarrow \\textbf{minimum} $ \n",
    "<br><br>\n",
    "$ f_{x_1 x_1 }  \\text { at }  ( \\frac{1}{4}(-6- \\sqrt{21}) ,  \\frac{-3}{16}(6 + \\sqrt{21}) ) =  12(\\frac{1}{4}(-\\sqrt{21} - 6)^2 + 24(\\frac{1}{4}(-\\sqrt{21} - 6) + 6) > 0 \\hspace{5mm}  and $$ f_{x_2 x_2 }  \\text { at } ( \\frac{1}{4}(-6- \\sqrt{21}) ,  \\frac{-3}{16}(6 + \\sqrt{21}) ) = 4 > 0   \\Rightarrow \\textbf{minimum} $ \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question2\n",
    ">$$ \\bullet \\textbf{ Steepst descent } \\bullet$$  <br>\n",
    "<br>\n",
    "**step 0** : Given $x^0$, set $k:=0.$ <br>\n",
    "**step 1** : $d^k := -\\Delta f(x^k).$ if $||d^k|| \\leq \\epsilon$ then stop.<br>\n",
    "**step 2** : Solve $min_\\lambda h(\\lambda) := f(x^k + \\lambda d^k) $for the step-length $ lambda^k $ , perhaps chosen by an exact or inexact line-search. <br>\n",
    "**step 3** : Set $x^{k+1} \\longleftarrow x^k + \\lambda^k d^k , k \\longleftarrow k+1$  \n",
    "$\\hspace{1.4cm}$Go to **Step 1** . \n",
    "<br>\n",
    "<br>\n",
    ">$$ \\bullet \\textbf{ Armijo Rule } \\bullet$$  <br>\n",
    "<br>\n",
    "Inexact line search method. Requires two parameters: $\\epsilon \\in (0,1) \\hspace{2mm},\\hspace{2mm} \\sigma > 1.$<br>\n",
    "$\\bar{h}(\\lambda) = h(0) + \\lambda \\epsilon h'(0)$.\n",
    "<br><bar>\n",
    "$\\bar{\\lambda}$ acceptable by Armijo'a rule if :<br><br>\n",
    "$ \\bullet \\hspace{2mm}  h(\\bar{\\lambda}) \\leq \\bar{h}(\\bar{\\lambda} )  $<br>\n",
    "$ \\bullet  \\hspace{2mm}  h(\\sigma \\bar{\\lambda}) \\geq \\bar{h}(\\sigma \\bar{\\lambda} )  $(Prevents the step size be small)<br>\n",
    "(i.e. $f(x^k + \\bar{\\lambda}d^k) \\leq f(x^k) + \\bar{\\lambda} \\epsilon \\Delta f(x^k)' d^k$)\n",
    "<br><br>\n",
    ">**step 0** : Set $k=0, \\lambda^0 = \\bar{\\lambda} > 0$<br>\n",
    "    >**step k** : If $h(\\lambda^k) \\leq \\bar{h}(\\lambda^k)$ choose $\\lambda^k$ stop.$\\hspace{3mm}$If $h(\\lambda^k) > \\bar{h}(\\lambda^k) $ let $\\lambda^{k+1} = \\frac{1}{\\sigma} \\lambda^k$( for exmaple, $\\sigma = 2$)<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### solution :\n",
    " \n",
    "\n",
    ">$$f(x) = 3x_1^2 + 2x_1 +8x_2 + 4x_2^2 $$\n",
    "<br>\n",
    "$\\textbf{part1}$\n",
    "<br><br>\n",
    " $d_k = - \\nabla f (x_1^k, x_2^k) = $ \n",
    "$\\bigg[^{-6x_1^k -2} _{-8-8x_2^k} \\bigg] = \\bigg[^{-2} _{-8} \\bigg]$\n",
    "<br><br>\n",
    "$\\textbf{part2}$\n",
    "<br><br>\n",
    "$f(0, 0) = 0 $<br> $d_k = - \\nabla f (x_1^k, x_2^k) = $ \n",
    "$\\bigg[^{-6x_1^k -2} _{-8-8x_2^k} \\bigg] =$ $\\bigg[^{d_1^k} _{d_2^k}\\bigg]  \\hspace{5mm}\\star$\n",
    "<br>\n",
    "<br>\n",
    "$h(\\lambda) = f(x^k + \\lambda d^k ) = 3(x_1^k + \\lambda d_1^k )^2 + 2(x_1^k + \\lambda d_1^k ) +8(x_2^k + \\lambda d_2^k ) + 4(x_2^k + \\lambda d_2^k )^2$\n",
    "<br> <br>\n",
    " $\\frac{ \\partial h(\\lambda) }{\\partial \\lambda} = 6d_1^k(x_1^k + \\lambda d_1^k) + 2d_1^k + 8d_2^k + 8(d_2^k)(x_2^k + \\lambda d_2^k ) \\Rightarrow \\lambda = \\frac{6d_1^kx_1^k + 2d_1^k + 8d_2^k + 8d_2^kx_2^k}{6(d_1^k)^2 + 8(d_2^k)^2}$ \n",
    " <br>\n",
    " if $x_1^0 = 0 \\hspace{2mm} , \\hspace{2mm}x_2^0 = 0 \\Rightarrow \\lambda = \\frac{2d_1^k + 8d_2^k}{6(d_1^k)^2 + 8(d_2^k)^2}  \\overset{\\star} {\\Longrightarrow}  \\hspace{2mm} {d_1^0 = -2 \\hspace{2mm} ,\\hspace{2mm}  d_2^0 = -8} \\hspace{2mm}  {\\Longrightarrow}  \\lambda = \\frac{2d_1^k + 8d_2^k}{6(d_1^k)^2 + 8(d_2^k)^2}  = 0.126$  \n",
    " <br>\n",
    " <br>\n",
    " $x_1^1 \\leftarrow x_1^0 +  \\lambda \\times d_1^0  \\hspace{2mm}  {\\Longrightarrow} x_1^1 = 0 + (0.12)(-2) = -0.24$\n",
    " <br> $x_2^1 \\leftarrow x_2^0 +  \\lambda \\times d_2^0  \\hspace{2mm}  {\\Longrightarrow} x_2^1 = 0 + (0.12)(-8) = -0.96$\n",
    " <br><br>\n",
    "$\\textbf{part3}$\n",
    "<br><br>\n",
    " $f(x^k + \\bar{\\lambda}d^k) \\leq f(x^k) + \\bar{\\lambda} \\epsilon \\nabla f(x^k)' d^k$\n",
    " <br><br>\n",
    " $\\epsilon = 0.1 \\hspace{4mm} , \\hspace{4mm}\\lambda^0 = \\frac{1}{|\\nabla f|} = \\frac{1}{8}$\n",
    " <br><br>\n",
    "  $f(x^0 + \\lambda^0 d^0) = f(\\frac{-1}{4}, -1) = -4.31 $<br>\n",
    "$f(x^0) + \\lambda^0  \\epsilon \\nabla f(x^0)^T d^0 = 0 + 0.1 \\times \\frac{1}{8} [2 \\hspace{4mm} 8]\\bigg[^{-2} _{-8} \\bigg] = -1.25 $\n",
    "$-4.31 \\leq -1.25 $ condition holds <br>\n",
    "<br><br>\n",
    "<br><br>\n",
    "$\\textbf{part4}$\n",
    "<br>\n",
    ">$$ \\bullet \\textbf{ Steepst descent } \\bullet$$  <br>\n",
    "$h(\\lambda) = f(x^k + \\lambda d^k ) = 3(x_1^k + \\lambda d_1^k )^2 + 2(x_1^k + \\lambda d_1^k ) +8(x_2^k + \\lambda d_2^k ) + 4(x_2^k + \\lambda d_2^k )^2$\n",
    "<br> <br>\n",
    " $\\frac{ \\partial h(\\lambda) }{\\partial \\lambda} = 6d_1^k(x_1^k + \\lambda d_1^k) + 2d_1^k + 8d_2^k + 8(d_2^k)(x_2^k + \\lambda d_2^k ) \\Rightarrow \\lambda = \\frac{6d_1^kx_1^k + 2d_1^k + 8d_2^k + 8d_2^kx_2^k}{6(d_1^k)^2 + 8(d_2^k)^2}$ \n",
    " <br>\n",
    " if $x_1^1 = -0.24 \\hspace{2mm} , \\hspace{2mm}x_2^1 = -0.96 \\Rightarrow \\lambda = \\frac{2d_1^k + 8d_2^k}{6(d_1^k)^2 + 8(d_2^k)^2}  \\overset{\\star} {\\Longrightarrow}  \\hspace{2mm} {d_1^0 = 3.44 \\hspace{2mm} ,\\hspace{2mm}  d_2^1 = 15.65} \\hspace{2mm}  {\\Longrightarrow}  \\lambda = \\frac{2d_1^k + 8d_2^k}{6(d_1^k)^2 + 8(d_2^k)^2}  = -0.126$  \n",
    " <br>\n",
    " <br>\n",
    " $x_1^2 \\leftarrow x_1^1 +  \\lambda \\times d_1^1  \\hspace{2mm}  {\\Longrightarrow} x_1^2 = -0.24 + (-0.12)() = 0.24$\n",
    " <br> $x_2^2 \\leftarrow x_2^1 +  \\lambda \\times d_2^1  \\hspace{2mm}  {\\Longrightarrow} x_2^2 = -0.96 + (-0.12)(-8) = 0.96$\n",
    " <br><br>\n",
    " >$$ \\bullet \\textbf{ Armijo Rule } \\bullet$$  <br>\n",
    " $f(x^k + \\bar{\\lambda}d^k) \\leq f(x^k) + \\bar{\\lambda} \\epsilon \\nabla f(x^k)T d^k$\n",
    " <br><br>\n",
    " $\\epsilon = 0.1 \\hspace{4mm} , \\hspace{4mm}\\lambda^0 = \\frac{1}{|\\nabla f|} = \\frac{1}{16}$\n",
    " <br><br>\n",
    "  $f(x^0 + \\lambda^0 d^0) = f(\\frac{-1}{8},\\frac{-1}{8} ) = -4.2 $<br>\n",
    "$f(x^0) + \\lambda^0  \\epsilon \\nabla f(x^0)^T d^0 = 0 + 0.1 \\times \\frac{1}{16} [2 \\hspace{4mm} 8]\\bigg[^{-2} _{-8} \\bigg] = -0.6 $<br>\n",
    "$-4.31 \\leq -0.6 $ condition holds <br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question3\n",
    " >$$ \\bullet \\textbf{ K-Fold Cross Validation } \\bullet$$  <br>\n",
    "In k-fold cross-validation, the original sample is randomly partitioned into k equal sized subsamples. Of the k subsamples, a single subsample is retained as the validation data for testing the model, and the remaining k − 1 subsamples are used as training data. The cross-validation process is then repeated k times, with each of the k subsamples used exactly once as the validation data. The k results can then be averaged to produce a single estimation.\n",
    " >$$ \\bullet \\textbf{ One vs. all } \\bullet$$  <br>\n",
    " One Vs. All is a heuristic method for using binary classification algorithms for multi-class classification. It involves splitting the multi-class dataset into multiple binary classification problems.\n",
    ">$$ \\bullet \\textbf{ Stochastic Gradient Descent } \\bullet$$  <br>\n",
    "SGD tries to solve the main problem in Batch Gradient descent which is the usage of whole training data to calculate gradients as each step. SGD is stochastic in nature i.e it picks up a “random” instance of training data at each step and then computes the gradient making it much faster as there is much fewer data to manipulate at a single time, unlike Batch GD.\n",
    ">$$ \\bullet \\textbf{ Batch Gradient Descent } \\bullet$$  <br>\n",
    "Batch Gradient Descent involves calculations over the full training set at each step as a result of which it is very slow on very large training data. Thus, it becomes very computationally expensive to do Batch GD. However, this is great for convex or relatively smooth error manifolds. Also, Batch GD scales well with the number of features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    ">**finding Optimum $\\lambda$ using SGD:**<br><br>\n",
    ">Although it was mentioned that finding the Optimum $\\lambda$ for only one class was enough, for the sake of better accuracy, i found the Optimum $\\lambda$ for all the classes, results are as followes: <br><br>\n",
    ">$$ Loss \\hspace{2mm} Function = \\lambda||w||^2 + \\frac{1}{n} \\sum_{i = 1}^n max(0, 1-y_i(wx_i - b))$$<br>\n",
    "In stochastic we take a random $x_k$ and substitude other datas $(x_i, k \\ne i)$ with it. \n",
    "<br>$$\\frac{\\partial}{\\partial w_k} \\lambda||w||^2 = 2\\lambda w_k$$\n",
    "<br>$$\\frac{\\partial}{\\partial w_k} max(0, 1-y_i(wx_i - b)) = \\bigg\\{^{0 \\hspace{10mm}y_i(wx_i - b)\t\\geq 1} _{-y_ix_{ik} \\hspace{3mm}y_i(wx_i - b)<1} $$\n",
    "updating $w$ : $$w =  \\bigg\\{^{w+ \\alpha (-2\\lambda w ) \\hspace{11mm}y_i(wx_i - b)\t\\geq 1} _{w+ \\alpha (-2\\lambda w + y_ix_i) \\hspace{3mm}y_i(wx_i - b)<1}$$\n",
    "Similarly fot $b$ :$$b = b-y_i \\hspace{4mm}if\\hspace{2mm}y_i(wx_i - b)<1 $$\n",
    "<br><br><br>\n",
    "<img src=\"lambda_table.png\"> <br><br>\n",
    "<img src=\"batch.png\"> \n",
    "<img src=\"stoch.png\" > <br><br>\n",
    "<img src=\"figures\\s_both (1).jpg\" width=\"400\" align=\"left\">\n",
    "<img src=\"figures\\fig4.png\" width=\"400\" align=\"right\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures\\trsin_s_const.JPG\" width=\"400\" align=\"left\">\n",
    "<img src=\"figures\\test_s_const.JPG\" width=\"400\" align=\"right\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures\\train_b_const.JPG\" width=\"400\" align=\"left\">\n",
    "<img src=\"figures\\test_b_const.JPG\" width=\"400\" align=\"right\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures\\train_b_decay.JPG\" width=\"400\" align=\"left\">\n",
    "<img src=\"figures\\test_b_decay.JPG\" width=\"400\" align=\"right\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures\\train_s_decay.JPG\" width=\"400\" align=\"left\">\n",
    "<img src=\"figures\\test_s_decay.JPG\" width=\"400\" align=\"right\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question4\n",
    "\n",
    ">$\\textbf{part1}$\n",
    "<br><br>\n",
    "$$Euclidean \\hspace{2mm} Distance = \\sqrt{\\sum_{i=1}^n{|x_i - y_i|^2}}$$\n",
    "**Note:**All these accuracies where measure after fitting the scaller.<br>\n",
    "I scaled my data by normalizing and standardizing\n",
    "As we can see, the best k is 5 which will be used in part 3.<br>\n",
    "<img src=\"k.png\" align=\"center\"> <br>\n",
    "To get better accuracy, we used normalizer that basicaly subtracts the mean of the data from each element and then divid it by data's variance.\n",
    "<br><br><br><br>\n",
    ">$\\textbf{part2}$\n",
    "<br><br>\n",
    "The best choice of k depends upon the data.<br>\n",
    "Generally,  a small value of k means that noise will have a higher influence on the result; although larger values of k reduces effect of the noise on the classification, but also make it computationally expensive and make boundaries between classes less distinct.\n",
    "<br>\n",
    "In addition, A small value of k could lead to overfitting as well as a big value of k can lead to underfitting. Overfitting imply that the model is well on the training data but has poor performance when new data is coming. Underfitting refers to a model that is not good on the training data and also cannot be generalized to predict new data.<br>\n",
    "We choose to have a maximum number of classifiers to be not greater than the square root of the training data set size. <br><br><br><br>\n",
    ">$\\textbf{part3}$\n",
    "<br><br>\n",
    "$$Manhattan \\hspace{2mm} Distance = d_2(x,y)= {\\sum_{i=1}^n{|x_i - y_i|}}$$\n",
    "$$Chebyshev \\hspace{2mm} Distance = d_1(x,y) = max _i{|x_i - y_i|}$$\n",
    "<img src=\"measures.png\" align=\"center\"> <br>\n",
    "<br><br><br><br>\n",
    ">$\\textbf{part4}$\n",
    "<br><br>\n",
    "The performance of KNN classifier depends significantly on the distance used.the results showed large gaps between the performances of different distances.)<br>\n",
    "We get similar classification results when we use distances from the same family having almost the same\n",
    "equation(*Chebyshev*, *Euclidean* and *Manhattan* all belong to **Minkowski**'s family of measures.), some distances are very similar, for example, one is twice the other, or one is the square of another.\n",
    "In these cases, and since the KNN compares examples using the same distance, the nearest neighbors will be\n",
    "the same if all distances were multiplied or divided by the same constant.<br><br><br>\n",
    "Your classes, using the features/measurements you chose to use, are badly overlapped. If you were able to plot each data point in 296-space, you'd see that there is a lot of mixing of where the classes occur.<br><br>\n",
    "To solve this problem we can try different approaches:<br>\n",
    "1.**Use more data**: Having more data is always a good idea. It allows the “data to tell for itself,” instead of relying on assumptions and weak correlations. Presence of more data results in better and accurate models.\n",
    "<br>\n",
    "2.**Algorithm Tuning**:We know that machine learning algorithms are driven by parameters. These parameters majorly influence the outcome of learning process.\n",
    "The objective of parameter tuning is to find the optimum value for each parameter to improve the accuracy of the model. \n",
    "<br><br>\n",
    "3. **Scaling**:The issue with sparsity is that it very biased or in statistical terms skewed. So, therefore, scaling the data brings all your values onto one scale eliminating the sparsity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Refrences\n",
    "https://www.geeksforgeeks.org/difference-between-batch-gradient-descent-and-stochastic-gradient-descent/\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}